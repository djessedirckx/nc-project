{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-53c7cb9e-70c1-42f7-9936-3efc6177ab43",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Genetic algorithm for Pacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-38209229-b1c6-4250-8b45-a8ed1f0ccad9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1623243853304,
    "source_hash": "9998acf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1146de30-d206-48d0-af84-60fede6d52ef",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13623,
    "execution_start": 1623243862839,
    "output_cleared": false,
    "source_hash": "f66757ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install atari_py\n",
    "!pip install gym\n",
    "!python -m atari_py.import_roms /work/ROMS/\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-135687d7-0189-4610-9385-79b62db38655",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5239,
    "execution_start": 1623243876472,
    "source_hash": "687a857",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# Import openai Gym \n",
    "import gym\n",
    "import atari_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-27cca4e7-313a-48f7-97f8-7530d42e959b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 789,
    "execution_start": 1623243881725,
    "source_hash": "f3f43e4b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing the environment\n",
    "env = gym.make('MsPacman-ram-v0')\n",
    "\n",
    "print('Observation-space:', env.observation_space)\n",
    "print('Available actions', env.unwrapped.get_action_meanings())\n",
    "\n",
    "render = lambda : plt.imshow(env.render(mode='rgb_array')) # plotting function\n",
    "plt.rcParams[\"figure.figsize\"] = (7,10)\n",
    "render()\n",
    "plt.savefig('ms-pacman.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-a415e22d-1cde-4c91-9499-b907d6176019",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 548,
    "execution_start": 1623243882523,
    "source_hash": "783b7786",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the RNN\n",
    "\n",
    "rnn = load_model('rnn_predictor_9_actions.hdf5')\n",
    "\n",
    "def predict_rnn(state, action, hidden, cell_values):\n",
    "    state_input = np.concatenate([state, [action]])\n",
    "\n",
    "    # Construct rnn input\n",
    "    rnn_input = [np.array([[state_input]]),np.array([hidden]),np.array([cell_values])]\n",
    "\n",
    "    # Predict hidden state given the current state and action\n",
    "    mdn, h, c = rnn.predict(rnn_input)\n",
    "    \n",
    "    return h[0], c[0]\n",
    "\n",
    "# Call the predictor\n",
    "\n",
    "def predict_controller(controller, hidden, ram):\n",
    "\n",
    "    concatenated_input = np.concatenate([hidden,ram])\n",
    "    concatenated_input = np.expand_dims(concatenated_input, axis=0)\n",
    "    \n",
    "    action = np.argmax(controller.predict(concatenated_input), axis=-1)[0]\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-289035d3-b10d-4e12-bcb1-cca58718ea41",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-f01a75f9-87ea-4514-8cdd-114023347a9c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1623243883095,
    "source_hash": "c9514826",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# genetic algorithm\n",
    "\n",
    "def choose_action(weights, combined):\n",
    "\n",
    "    '''\n",
    "    Choose an action:\n",
    "        input - weights and environment state\n",
    "        output - action (either left or right)\n",
    "    '''\n",
    "    \n",
    "    # action depends on sum of weights*inputs\n",
    "    results = sum(weights*combined)/100\n",
    "\n",
    "    # choose action\n",
    "    if results > 42 and results <= 44:\n",
    "        action = 1\n",
    "    elif results > 44 and results <= 46:\n",
    "        action = 2\n",
    "    elif results > 46 and results <= 48:\n",
    "        action = 3\n",
    "    elif results > 48 and results <= 50:\n",
    "        action = 4\n",
    "    elif results > 50 and results <= 52:\n",
    "        action = 5\n",
    "    elif results > 52 and results <= 54:\n",
    "        action = 6\n",
    "    elif results > 52 and results <= 54:\n",
    "        action = 7\n",
    "    elif results > 54 and results <= 56:\n",
    "        action = 8\n",
    "    else: \n",
    "        action = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def evaluate_agent(weights):\n",
    "\n",
    "    '''\n",
    "    Evaluate the current weight configuration (the agent):\n",
    "        input - weights\n",
    "        output - reward associated with the input values, average action\n",
    "    '''\n",
    "\n",
    "    # collector\n",
    "    total_reward = 0\n",
    "    total_action = 0\n",
    "    counter_action = 0\n",
    "    action_collector = []\n",
    "\n",
    "    # set the environment\n",
    "    env.reset()\n",
    "    state, reward, done, info = env.step(1)\n",
    "    \n",
    "    runs = 5\n",
    "    action = 1 # first action\n",
    "    hidden = np.zeros(256)\n",
    "    cell_values = np.zeros(256)\n",
    "    \n",
    "    for i in range(runs):\n",
    "\n",
    "        # stop game when done-flag is raised\n",
    "        while done != True:\n",
    "\n",
    "\n",
    "\n",
    "            # use concatenated RNN + state\n",
    "            state, reward, done, info = env.step(action)   # take action\n",
    "\n",
    "            prediction_h, predict_c  = predict_rnn(state, action, hidden, cell_values)\n",
    "            hidden = prediction_h\n",
    "            cell_values = predict_c\n",
    "\n",
    "            combined = np.concatenate([state, prediction_h])\n",
    "            action = choose_action(weights, combined)         # choose action\n",
    "\n",
    "            total_reward += reward                         # accumulate rewards\n",
    "            total_action += action                         # track taken actions    \n",
    "            counter_action += 1\n",
    "        \n",
    "            action_collector.append(action)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        env.reset()\n",
    "        state, reward, done, info = env.step(1)\n",
    "    \n",
    "    # compute the average action and the average reward\n",
    "    avg_action = total_action/counter_action\n",
    "    avg_reward = total_reward/runs\n",
    "\n",
    "    # commmand line outputs\n",
    "    print('Total reward', avg_reward)\n",
    "    print('Avg. action', avg_action)\n",
    "\n",
    "    return total_reward/runs, avg_action, action_collector\n",
    "\n",
    "\n",
    "def genetic_agent(runs):\n",
    "\n",
    "    '''\n",
    "    Genetic Agent (main loop):\n",
    "        input - amount of runs, possibly old weights\n",
    "        output - monitoring variables + save new weights\n",
    "    '''\n",
    "\n",
    "    print('Total number of runs:', runs)\n",
    "  \n",
    "    highest_rewards = []\n",
    "    episode_of_change = []\n",
    "\n",
    "    weights = 0.76*np.random.random(384)\n",
    "    reward_old = -500\n",
    "    #data = np.load('img-GA/pacman-1000-weights-sigma1.2-sparse-action4-evaldepth5.npz')\n",
    "    #weights = data['arr_0']\n",
    "    weights_old = weights\n",
    "    reward_old, _, _ = evaluate_agent(weights_old)\n",
    "\n",
    "    highest_rewards.append(reward_old)\n",
    "\n",
    "    generation = 1\n",
    "    print('Generation:', generation)\n",
    "\n",
    "    generation_distance = []\n",
    "    old_generation = 0\n",
    "    action_collector = []\n",
    "    total_action_collector = []\n",
    "    reward_collector = []\n",
    "    best_generation_action = []\n",
    "    elapse_time_col = []\n",
    "\n",
    "    for i in range(runs):\n",
    "\n",
    "        total_action_collector = []\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        print('')\n",
    "        print('Round', i)\n",
    "        \n",
    "        # mutation\n",
    "        w_pos = 0.8*np.random.random(384)\n",
    "        w_neg = -0.8*np.random.random(384)\n",
    "        w_update = w_pos + w_neg\n",
    "\n",
    "        w_update = w_update*np.random.randint(2, size=384) # sparse input\n",
    "\n",
    "        weights_new = weights_old + w_update\n",
    "\n",
    "        reward_new, avg_action, actions = evaluate_agent(weights_new)\n",
    "\n",
    "        total_action_collector.append(actions)\n",
    "        action_collector.append(avg_action)\n",
    "        reward_collector.append(reward_new)\n",
    "\n",
    "        current_generation = i\n",
    "\n",
    "        elapsed = time.time() - t\n",
    "        print('Elapse time', round(elapsed, 3))\n",
    "        elapse_time_col.append(elapsed)\n",
    "\n",
    "        if i == 0:\n",
    "            best_generation_action.append(total_action_collector[0])\n",
    "\n",
    "        if i%100 == 0:\n",
    "            print('weights old', weights_old)\n",
    "            print('weight update', w_update)\n",
    "\n",
    "        # check whether new weight configuration is better than the old one\n",
    "        if reward_new > reward_old:\n",
    "\n",
    "            weights_old = weights_new   # update weights\n",
    "            reward_old = reward_new     # update best reward\n",
    "            generation += 1             # increase generation\n",
    "\n",
    "            # collect results for plotting\n",
    "\n",
    "            highest_rewards.append(reward_new)\n",
    "            generation_distance.append(current_generation - old_generation)\n",
    "            episode_of_change.append(current_generation)\n",
    "            best_generation_action.append(total_action_collector[0])\n",
    "\n",
    "            # commmand line outputs\n",
    "            print('')\n",
    "            print('########## WEIGHT CHANGE ############')\n",
    "            print('Generation:', generation)\n",
    "            print('Rounds betw. generations:', current_generation - old_generation)\n",
    "            \n",
    "            old_generation = current_generation\n",
    "    \n",
    "    # save final weights\n",
    "    np.savez('pacman-1000-weights-sigma0.8-sparse-RNN-action4-evaldepth5.npz', weights_old)\n",
    "\n",
    "    return highest_rewards, generation_distance, weights_old, action_collector, reward_collector, episode_of_change, best_generation_action, elapse_time_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-dd165ebe-24b2-4742-8c28-cd8fd84bb3f5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17447,
    "execution_start": 1623243883109,
    "source_hash": "263ad360",
    "tags": []
   },
   "outputs": [],
   "source": [
    "collect = []\n",
    "\n",
    "# compute the average result value to calibrate the action score\n",
    "for i in range(1000):\n",
    "    env.reset()\n",
    "    state, reward, done, info = env.step(1)\n",
    "    w = 0.74*np.random.random(len(state))\n",
    "    collect.append(sum(state*w)/100)\n",
    "    # print(sum(state*weights)/100)\n",
    "\n",
    "print(sum(collect)/len(collect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-5a7f416a-ce23-4858-bb4b-57904a69371d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Running the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-81852fe6-aca9-491a-9909-b4e3c603db43",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1623243900553,
    "source_hash": "6797d8d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "env = gym.make('MsPacman-ram-v0')\n",
    "env.reset()\n",
    "state, reward, done, info = env.step(0)\n",
    "\n",
    "# run the genetic algorithm\n",
    "highest_rewards, gen_dis, weights, actions, all_rewards, change_episode, all_actions, elapse_time  = genetic_agent(runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-fe30c850-77ea-480a-ae5e-9919298a2828",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 22,
    "execution_start": 1623155177897,
    "source_hash": "a40cd1bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_rewards\n",
    "\n",
    "mean_reward = sum(all_rewards)/len(all_rewards)\n",
    "\n",
    "counter = 0\n",
    "for i in all_rewards:\n",
    "    counter += (i - mean_reward) ** 2\n",
    "\n",
    "variance = counter / len(all_rewards)\n",
    "std = variance ** 0.5\n",
    "\n",
    "print(all_rewards)\n",
    "\n",
    "print('mean', mean_reward)\n",
    "print('variance', variance)\n",
    "print('std', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-265afd38-0b03-4bd9-bfaa-db1afafc556c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-114553de-b57f-45fa-acc3-c6ff7c27262c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 292,
    "execution_start": 1623154741601,
    "source_hash": "45f6f1d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(all_actions[0], return_counts=True)\n",
    "# distribution = dict(zip(unique, counts))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Create a figure instance\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "data_to_plot = []\n",
    "for i in range(len(all_actions)):\n",
    "    data_to_plot.append(all_actions[i])\n",
    "\n",
    "# Create a figure instance\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "# Create the boxplot\n",
    "bp = ax.violinplot(data_to_plot, showmeans = True, showextrema = True, showmedians = True)\n",
    "\n",
    "for partname in ('cbars','cmins','cmaxes','cmeans','cmedians'):\n",
    "    vp = bp[partname]\n",
    "    vp.set_edgecolor('slategray')\n",
    "    vp.set_linewidth(1)\n",
    "\n",
    "for pc in bp['bodies']:\n",
    "    pc.set_facecolor('slategray')\n",
    "    pc.set_edgecolor('slategray')\n",
    "    pc.set_alpha(1)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Best Generation') \n",
    "plt.ylabel('Probability per action')\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, len(data_to_plot) +1, 1))\n",
    "plt.title(\"Action distribution\") \n",
    "plt.tight_layout()\n",
    "plt.savefig('img-GA/pacman-action-distr-1000-sigma0.8-RNN-sparse-action4-evaldepth5.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-55d5b11c-f7f7-4400-ae21-1a6be05076f2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1055,
    "execution_start": 1623154741929,
    "source_hash": "63a2efb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "# plot the obtained results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "# Reward improvement per generation\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(highest_rewards, color='slategray', marker = 'o', markersize=12)\n",
    "#plt.style.use('ggplot')\n",
    "plt.grid()\n",
    "plt.xticks(range(0, len(gen_dis)+1, 1))\n",
    "plt.xlabel('Generation') \n",
    "plt.ylabel('Obtained reward') \n",
    "plt.title(\"Best reward obtained by each Generation\")\n",
    "plt.savefig('img-GA/pacman-reward-1000-sigma0.8-RNN-sparse-action4-evaldepth5.pdf')\n",
    "plt.show() \n",
    "\n",
    "# Distance between generations\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(1, len(gen_dis)+1, 1), gen_dis, color='slategray')\n",
    "#plt.style.use('ggplot')\n",
    "plt.grid()\n",
    "plt.xticks(range(1, len(gen_dis)+1, 1))\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Mutations between Generations')\n",
    "plt.title(\"Distance between Generations\")\n",
    "plt.savefig('img-GA/pacman-distance-1000-simga0.8-RNN-sparse-action4-evaldepth5.pdf')\n",
    "plt.show() \n",
    "\n",
    "\n",
    "# Plot all rewards\n",
    "\n",
    "# change red and gray colour?\n",
    "window=20\n",
    "xcoords = change_episode\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "rolling_mean = pd.Series(all_rewards).rolling(window).mean()\n",
    "std = pd.Series(all_rewards).rolling(window).std()\n",
    "plt.grid()\n",
    "plt.plot(rolling_mean)\n",
    "plt.fill_between(range(len(all_rewards)),rolling_mean-std, rolling_mean+std, color='orange', alpha=0.4)\n",
    "# plt.axvline(x=30, color='slategray', linestyle='-')\n",
    "\n",
    "for xc in zip(xcoords):\n",
    "    plt.axvline(x=xc, label='line at x = {}'.format(xc), c='slategray')\n",
    "\n",
    "plt.title('Reward Moving Average ({}-episode window)'.format(window))\n",
    "plt.grid()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid()\n",
    "plt.savefig('img-GA/pacman-reward-over-1000-sigma0.8-RNN-sparse-action4-evaldepth5.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Plot all action\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "rolling_mean = pd.Series(actions).rolling(window).mean()\n",
    "std = pd.Series(actions).rolling(window).std()\n",
    "plt.plot(rolling_mean)\n",
    "plt.fill_between(range(len(actions)),rolling_mean-std, rolling_mean+std, color='orange', alpha=0.4)\n",
    "\n",
    "for xc in zip(xcoords):\n",
    "    plt.axvline(x=xc, label='line at x = {}'.format(xc), c='slategray')\n",
    "\n",
    "plt.title('Actions Moving Average ({}-episode window)'.format(window))\n",
    "plt.grid()\n",
    "plt.savefig('img-GA/pacman-actions-over-1000-sigma0.8-RNN-sparse-action4-evaldepth5.pdf', bbox_inches='tight')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Action')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-41f21a4e-24e3-437b-a522-bb49e2836b89",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 345,
    "execution_start": 1623154742986,
    "source_hash": "7cd02ed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "total = sum(elapse_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(elapse_time, color='slategray')\n",
    "plt.xlabel('Episode') \n",
    "plt.ylabel('Time')\n",
    "plt.title(\"Elapse time (total \" + str(round(total, 2)) + \" sec.)\" ) \n",
    "plt.grid()\n",
    "plt.savefig('img-GA/pacman-elapse-time-1000-sigma0.8-RNN-sparse-action4-evaldepth5.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "843a8baf-d4f3-4811-b68b-0a99b96e840c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
